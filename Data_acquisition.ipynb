{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18372d5e",
   "metadata": {},
   "source": [
    "### Part 2: Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f727be4",
   "metadata": {},
   "source": [
    "The National Student Survey gathers feedback from students concerning student satisfaction in regards to their course, teaching, and other aspects of university. It's collected from final year undergraduate students across UK universities.\n",
    "\n",
    "Below, we filter data from the National Student Survey, as well as from the website Reddit.com. We also import all of the modules we will use throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7fa884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46406f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lse_dat=pd.read_excel('./data/LSE_dat.xlsx', sheet_name=\"Teaching\")\n",
    "lse_dat.columns = lse_dat.iloc[0]\n",
    "lse_dat = lse_dat[1:]  #we do this to remove the top row of the data, which is, redundantly, the same as the next row\n",
    "lse_dat\n",
    "lse_dat.to_csv(\"Data/lse_dat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a47824",
   "metadata": {},
   "source": [
    "There is also data from other universities that we have to similarly process and store into dataframes. For this project we have chosen to compare LSE to the following:\n",
    "\n",
    "- UCL\n",
    "- KCL\n",
    "- SOAS\n",
    "- Loughborough University\n",
    "\n",
    "more universities needed/might use SQL here somehow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648ae4",
   "metadata": {},
   "source": [
    "For the Reddit API data, we decided to focus on the subreddit 'UniUK' that has 151 thousand members and is in the top 1% of communities ranked by size. We noticed few posts directly used the phrase 'student satisfaction' when discussing LSE, which we assumed was due to the informal nature of the site. \n",
    "\n",
    "Instead, we searched for posts that contained the words 'lse' and 'opinion'. We retreived the top thirty posts under this search criteria, and stored the 'Post Title', 'Score' (number of 'upvotes' by other redditers) and 'Top Comment' in a pandas data frame 'df'. This dataframe was then saved to a csv file \"Data/reddit_data.csv\" so that it can accessed from any other files, and crucially, so that the data only needs to be fetched from Reddit.com once. Since the contents of the subreddit is likely to change frequently, it would be inefficent to perform effective analysis using realtime data, so we instead stored data from the site on 12/04/2024.\n",
    "\n",
    "We chose to use the website Reddit to acquire data for many reasons. Firstly, their API is easy to use, reliable and openly accessible to the public. This allowed us to effiently access user-generated content without the computational and legal issues that web scraping would risk. Since the type of textual data we are searching for (the general opinion of LSE) is rarely provided in formatted tables online, using an the Reddit API is the best way to acquire and aggregate this information in a structured way.\n",
    "\n",
    "Furthermore, the website has a strong user base for university students or people generally interested in the subject, exemplified by the size of the subreddit 'UniUK'. Not only does this give access to a large amount of data, but the posts are detailed and honest. This is because a significant part of the branding for Reddit is the idea of community, where you are likely to find other users who want contribute to the discussion under the protection of anonymity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f5d01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reddit API Data Collection.\n",
    "Use the API key to request an access token by using the data stored in keys.json.\n",
    "Make an API call to retrive posts from 'UniUK'\n",
    "Store the ddata in a data frame.\n",
    "'''\n",
    "\n",
    "#access keys\n",
    "with open('Data/keys.json') as f:\n",
    "    keys = json.load(f)\n",
    "app_id = keys['reddit']['app_id']\n",
    "app_secret = keys['reddit']['app_secret']\n",
    "username = keys['reddit']['username']\n",
    "password = keys['reddit']['password']\n",
    "\n",
    "#request a token\n",
    "client_auth = requests.auth.HTTPBasicAuth(app_id, app_secret)\n",
    "post_data = {'grant_type': 'password',\n",
    "            'username': username,\n",
    "            'password': password}\n",
    "headers = {'User-Agent': f'new connection lse/0.0.1 by {username}'}\n",
    "r = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=client_auth, data=post_data, headers=headers)\n",
    "access_token = r.json()['access_token']\n",
    "\n",
    "#send a request\n",
    "headers = {\"Authorization\": f\"bearer {access_token}\",\n",
    "'User-Agent': f'lse/0.0.1 by {username}'}\n",
    "\n",
    "r = requests.get(\"https://oauth.reddit.com/r/UniUK/search?q=lse\" \\\n",
    "\"&limit=30&sort=top&restrict_sr=true\", headers=headers)\n",
    "\n",
    "titles = []\n",
    "scores = []\n",
    "top_comments = []\n",
    "\n",
    "#post title, score and top comment\n",
    "for post in r.json()[\"data\"][\"children\"]:\n",
    "    post_title = post[\"data\"][\"title\"]\n",
    "    post_score = post[\"data\"][\"score\"]\n",
    "    post_permalink = post[\"data\"][\"permalink\"]\n",
    "\n",
    "    comments_request = requests.get(f\"https://oauth.reddit.com{post_permalink}.json\", headers=headers)\n",
    "    comments_data = comments_request.json()\n",
    "\n",
    "    if isinstance(comments_data, list) and len(comments_data) > 1:\n",
    "        comments = comments_data[1][\"data\"][\"children\"]\n",
    "        if comments:\n",
    "            top_comment = comments[0][\"data\"][\"body\"]\n",
    "            titles.append(post_title)\n",
    "            scores.append(post_score)\n",
    "            top_comments.append(top_comment)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Score\": scores,\n",
    "    \"Top Comment\": top_comments\n",
    "})\n",
    "\n",
    "#save dataframe to csv file\n",
    "df.to_csv(\"Data/reddit_data.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
